{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CKA Analysis for LLM Models\n",
        "\n",
        "Quick-start guide for analyzing layer representations in transformer-based language models.\n",
        "\n",
        "**Requirements:** `pip install pytorch-cka transformers torch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    BertConfig, BertModel,\n",
        "    GPT2Config, GPT2Model,\n",
        "    DistilBertConfig, DistilBertModel,\n",
        ")\n",
        "\n",
        "from pytorch_cka import CKA, CKAConfig\n",
        "from pytorch_cka.viz import plot_cka_heatmap, plot_cka_trend, plot_cka_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DictDataset:\n",
        "    \"\"\"Simple dataset returning dict batches with input_ids.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_text_dataloader(batch_size=8, num_samples=32, seq_length=64, vocab_size=1000):\n",
        "    \"\"\"Create dataloader with random token IDs (synthetic data).\"\"\"\n",
        "    input_ids = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "    attention_mask = torch.ones(num_samples, seq_length, dtype=torch.long)\n",
        "    dataset = DictDataset(input_ids, attention_mask)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bert_layers(model, max_layers=12):\n",
        "    \"\"\"Extract BERT encoder layer names.\"\"\"\n",
        "    return [f\"encoder.layer.{i}\" for i in range(min(model.config.num_hidden_layers, max_layers))]\n",
        "\n",
        "\n",
        "def get_gpt2_layers(model, max_layers=12):\n",
        "    \"\"\"Extract GPT-2 transformer block layer names.\"\"\"\n",
        "    return [f\"h.{i}\" for i in range(min(model.config.n_layer, max_layers))]\n",
        "\n",
        "\n",
        "def get_distilbert_layers(model, max_layers=12):\n",
        "    \"\"\"Extract DistilBERT transformer layer names.\"\"\"\n",
        "    return [f\"transformer.layer.{i}\" for i in range(min(model.config.n_layers, max_layers))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global configuration\n",
        "BATCH_SIZE = 8          # Must be > 3 for unbiased HSIC\n",
        "NUM_SAMPLES = 32        # 4 batches\n",
        "SEQ_LENGTH = 64         # Sequence length\n",
        "VOCAB_SIZE = 1000       # Shared vocab size\n",
        "\n",
        "# CKA config with float64 for numerical stability\n",
        "cka_config = CKAConfig(\n",
        "    kernel=\"linear\",\n",
        "    unbiased=True,\n",
        "    dtype=torch.float64,\n",
        ")\n",
        "\n",
        "# Create shared dataloader\n",
        "dataloader = create_text_dataloader(BATCH_SIZE, NUM_SAMPLES, SEQ_LENGTH, VOCAB_SIZE)\n",
        "print(f\"Created dataloader with {len(dataloader)} batches of size {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Self-Similarity Analysis\n",
        "\n",
        "Analyze how layer representations relate within a single BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small BERT configuration (no pretrained weights download)\n",
        "bert_config = BertConfig(\n",
        "    hidden_size=256,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=512,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    max_position_embeddings=128,\n",
        ")\n",
        "bert = BertModel(bert_config)\n",
        "\n",
        "# Get encoder layer names\n",
        "bert_layers = get_bert_layers(bert)\n",
        "print(f\"BERT layers: {bert_layers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with CKA(bert, layers1=bert_layers, model1_name=\"BERT\", config=cka_config) as cka:\n",
        "    bert_self_cka = cka.compare(dataloader, progress=True)\n",
        "\n",
        "print(f\"Matrix shape: {bert_self_cka.shape}\")\n",
        "print(f\"Diagonal values (should be ~1.0): {torch.diag(bert_self_cka)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plot_cka_heatmap(\n",
        "    bert_self_cka,\n",
        "    layers1=bert_layers,\n",
        "    layers2=bert_layers,\n",
        "    model1_name=\"BERT\",\n",
        "    model2_name=\"BERT\",\n",
        "    title=\"BERT Self-Similarity\",\n",
        "    annot=True,\n",
        "    annot_fmt=\".2f\",\n",
        "    mask_upper=True,\n",
        "    layer_name_depth=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cross-Architecture Comparison\n",
        "\n",
        "Compare encoder (BERT) vs decoder (GPT-2) architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small GPT-2 configuration\n",
        "gpt2_config = GPT2Config(\n",
        "    n_embd=256,\n",
        "    n_layer=6,\n",
        "    n_head=4,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    n_positions=128,\n",
        ")\n",
        "gpt2 = GPT2Model(gpt2_config)\n",
        "\n",
        "# Get transformer block names\n",
        "gpt2_layers = get_gpt2_layers(gpt2)\n",
        "print(f\"GPT-2 layers: {gpt2_layers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with CKA(\n",
        "    bert, gpt2,\n",
        "    layers1=bert_layers,\n",
        "    layers2=gpt2_layers,\n",
        "    model1_name=\"BERT\",\n",
        "    model2_name=\"GPT-2\",\n",
        "    config=cka_config,\n",
        ") as cka:\n",
        "    cross_arch_cka = cka.compare(dataloader, progress=True)\n",
        "\n",
        "print(f\"Matrix shape: {cross_arch_cka.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plot_cka_heatmap(\n",
        "    cross_arch_cka,\n",
        "    layers1=bert_layers,\n",
        "    layers2=gpt2_layers,\n",
        "    model1_name=\"BERT\",\n",
        "    model2_name=\"GPT-2\",\n",
        "    title=\"BERT vs GPT-2\",\n",
        "    annot=True,\n",
        "    layer_name_depth=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Size Comparison\n",
        "\n",
        "Compare DistilBERT (3 layers) vs BERT (6 layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DistilBERT configuration (fewer layers than BERT)\n",
        "distilbert_config = DistilBertConfig(\n",
        "    dim=256,\n",
        "    n_layers=3,  # Half the layers of our BERT\n",
        "    n_heads=4,\n",
        "    hidden_dim=512,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    max_position_embeddings=128,\n",
        ")\n",
        "distilbert = DistilBertModel(distilbert_config)\n",
        "\n",
        "# Get transformer layer names\n",
        "distilbert_layers = get_distilbert_layers(distilbert)\n",
        "print(f\"DistilBERT layers: {distilbert_layers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with CKA(\n",
        "    bert, distilbert,\n",
        "    layers1=bert_layers,\n",
        "    layers2=distilbert_layers,\n",
        "    model1_name=\"BERT\",\n",
        "    model2_name=\"DistilBERT\",\n",
        "    config=cka_config,\n",
        ") as cka:\n",
        "    size_cka = cka.compare(dataloader, progress=True)\n",
        "\n",
        "print(f\"Matrix shape: {size_cka.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plot_cka_heatmap(\n",
        "    size_cka,\n",
        "    layers1=bert_layers,\n",
        "    layers2=distilbert_layers,\n",
        "    model1_name=\"BERT (6 layers)\",\n",
        "    model2_name=\"DistilBERT (3 layers)\",\n",
        "    title=\"Model Size Comparison\",\n",
        "    annot=True,\n",
        "    layer_name_depth=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pre-trained vs Fine-tuned Comparison\n",
        "\n",
        "Simulate fine-tuning by perturbing model weights in later layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy and perturb weights to simulate fine-tuning\n",
        "bert_finetuned = copy.deepcopy(bert)\n",
        "\n",
        "# Add noise to later layers (simulating task-specific fine-tuning)\n",
        "with torch.no_grad():\n",
        "    for name, param in bert_finetuned.named_parameters():\n",
        "        if \"encoder.layer.4\" in name or \"encoder.layer.5\" in name:\n",
        "            param.add_(torch.randn_like(param) * 0.1)\n",
        "\n",
        "print(\"Created fine-tuned model with perturbed layers 4-5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with CKA(\n",
        "    bert, bert_finetuned,\n",
        "    layers1=bert_layers,\n",
        "    layers2=bert_layers,\n",
        "    model1_name=\"Pre-trained\",\n",
        "    model2_name=\"Fine-tuned\",\n",
        "    config=cka_config,\n",
        ") as cka:\n",
        "    finetune_cka = cka.compare(dataloader, progress=True)\n",
        "\n",
        "# Diagonal shows layer-wise similarity before/after fine-tuning\n",
        "print(f\"Layer-wise similarity: {torch.diag(finetune_cka)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plot_cka_heatmap(\n",
        "    finetune_cka,\n",
        "    layers1=bert_layers,\n",
        "    layers2=bert_layers,\n",
        "    model1_name=\"Pre-trained\",\n",
        "    model2_name=\"Fine-tuned\",\n",
        "    title=\"Pre-trained vs Fine-tuned BERT\",\n",
        "    annot=True,\n",
        "    layer_name_depth=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plot_cka_trend(\n",
        "    torch.diag(finetune_cka),\n",
        "    labels=[\"Layer Similarity\"],\n",
        "    xlabel=\"Layer Index\",\n",
        "    ylabel=\"CKA Similarity\",\n",
        "    title=\"Fine-tuning Impact by Layer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Multi-Panel Comparison\n",
        "\n",
        "Compare all scenarios side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute GPT-2 self-similarity for completeness\n",
        "with CKA(gpt2, layers1=gpt2_layers, model1_name=\"GPT-2\", config=cka_config) as cka:\n",
        "    gpt2_self_cka = cka.compare(dataloader, progress=False)\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plot_cka_comparison(\n",
        "    matrices=[bert_self_cka, gpt2_self_cka, cross_arch_cka, finetune_cka],\n",
        "    titles=[\"BERT Self\", \"GPT-2 Self\", \"BERT vs GPT-2\", \"Pre vs Fine-tuned\"],\n",
        "    ncols=2,\n",
        "    share_colorbar=True,\n",
        "    figsize=(12, 10),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Observations\n",
        "\n",
        "1. **Self-similarity**: Diagonal values ~1.0; nearby layers show higher similarity\n",
        "2. **Cross-architecture**: Despite different designs, similar depth layers may share representations\n",
        "3. **Model size**: DistilBERT layers may correspond to multiple BERT layers\n",
        "4. **Fine-tuning**: Early layers remain stable; task-specific adaptations occur in later layers"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
